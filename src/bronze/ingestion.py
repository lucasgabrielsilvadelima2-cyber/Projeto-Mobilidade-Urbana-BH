"""
Camada Bronze - Ingest√£o de Dados Brutos.

Este m√≥dulo √© respons√°vel por extrair dados das APIs de dados abertos de
Belo Horizonte e salv√°-los em formato Parquet na camada Bronze (dados imut√°veis).
"""

import json
import logging
import os
from datetime import datetime
from typing import Any, Dict, List, Optional

import pandas as pd
from curl_cffi import requests
from curl_cffi.requests import Session

from ..utils.common import DataLineage, get_partition_path

logger = logging.getLogger(__name__)


class BronzeDataIngester:
    """Classe base para ingest√£o de dados na camada Bronze."""
    
    def __init__(self, output_path: str = "./data/bronze"):
        """
        Inicializa o ingestor de dados.
        
        Args:
            output_path: Caminho para salvar os dados brutos
        """
        self.output_path = output_path
        self.session = self._create_session()
        os.makedirs(output_path, exist_ok=True)
    
    def _create_session(self) -> Session:
        """
        Cria uma sess√£o HTTP com curl_cffi para emula√ß√£o de navegador.
        
        Usa curl_cffi ao inv√©s de requests padr√£o para contornar fingerprinting
        TLS/SSL que bloqueia requisi√ß√µes Python. Emula perfeitamente navegadores
        reais, funcionando em qualquer plataforma (Windows, Linux, macOS, Docker).
        
        Returns:
            Sess√£o configurada com browser impersonation
        """
        session = Session()
        
        # Headers b√°sicos - curl_cffi j√° emula headers de navegador
        session.headers.update({
            'Accept': '*/*',
            'Accept-Language': 'pt-BR,pt;q=0.9',
            'Referer': 'https://temporeal.pbh.gov.br/',
        })
        
        return session
    
    def _save_to_parquet(
        self,
        data: pd.DataFrame,
        dataset_name: str,
        partition_by_date: bool = True
    ) -> str:
        """
        Salva dados em formato Parquet.
        
        Args:
            data: DataFrame a ser salvo
            dataset_name: Nome do dataset
            partition_by_date: Se True, particiona por data
        
        Returns:
            Caminho do arquivo salvo
        """
        if partition_by_date:
            partition_path = get_partition_path(
                os.path.join(self.output_path, dataset_name)
            )
        else:
            partition_path = os.path.join(self.output_path, dataset_name)
            os.makedirs(partition_path, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = os.path.join(
            partition_path,
            f"{dataset_name}_{timestamp}.parquet"
        )
        
        data.to_parquet(
            file_path,
            engine="pyarrow",
            compression="snappy",
            index=False
        )
        
        logger.info(f"Dados salvos em: {file_path}")
        logger.info(f"Total de registros: {len(data)}")
        
        return file_path


class OnibusTempoRealIngester(BronzeDataIngester):
    """Ingestor de dados de √¥nibus em tempo real."""
    
    def __init__(
        self,
        api_url: str = "https://temporeal.pbh.gov.br/v1/posicoes",
        output_path: str = "./data/bronze"
    ):
        """
        Inicializa o ingestor de dados de √¥nibus.
        
        Args:
            api_url: URL da API de tempo real
            output_path: Caminho de sa√≠da
        """
        super().__init__(output_path)
        self.api_url = api_url
    
    def _fetch_data(self) -> str:
        """
        Faz requisi√ß√£o √† API com emula√ß√£o de navegador real.
        
        Usa curl_cffi para emular fingerprint TLS de navegadores reais,
        contornando prote√ß√µes WAF de forma leg√≠tima e port√°vel.
        Funciona em qualquer plataforma (Windows, Linux, macOS, Docker).
        
        Estrat√©gia: Tenta m√∫ltiplos perfis de navegador at√© encontrar um que funcione.
        
        Returns:
            Conte√∫do da resposta como string
            
        Raises:
            Exception: Se falhar ap√≥s todas tentativas
        """
        # Lista de impersonations para tentar (em ordem de compatibilidade)
        impersonations = [
            "chrome110",   # Chrome moderno
            "chrome107",   # Chrome um pouco mais antigo
            "safari15_5",  # Safari (bom para APIs Apple-friendly)
            "firefox109",  # Firefox alternativo
        ]
        
        last_error = None
        
        for impersonate in impersonations:
            try:
                logger.info(f"üîÑ Tentando acessar API (emulando {impersonate})...")
                
                response = requests.get(
                    self.api_url,
                    impersonate=impersonate,
                    timeout=30
                )
                
                # Verifica status code
                if response.status_code == 200:
                    logger.info(f"‚úÖ Sucesso com {impersonate} (status: {response.status_code})")
                    return response.text
                else:
                    logger.warning(f"‚ö†Ô∏è Status {response.status_code} com {impersonate}, tentando pr√≥ximo...")
                    last_error = f"HTTP {response.status_code}"
                    continue
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro com {impersonate}: {e}")
                last_error = e
                continue
        
        # Se todas tentativas falharem
        error_msg = f"N√£o foi poss√≠vel acessar API ap√≥s tentar {len(impersonations)} navegadores diferentes"
        logger.error(f"‚ùå {error_msg}. √öltimo erro: {last_error}")
        raise Exception(f"{error_msg}: {last_error}")
    
    def extract(self) -> pd.DataFrame:
        """
        Extrai dados de posicionamento dos √¥nibus.
        
        Returns:
            DataFrame com os dados extra√≠dos
        """
        lineage = DataLineage(
            source="PBH Tempo Real API",
            operation="extract_onibus_posicoes"
        )
        
        try:
            logger.info(f"üîÑ Extraindo dados de: {self.api_url}")
            
            # Usa m√©todo robusto com emula√ß√£o de navegador
            text_content = self._fetch_data()
            lineage.add_metadata("method", "curl_cffi_browser_impersonation")
            
            logger.debug(f"‚úì Dados recebidos com sucesso")
        
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair dados: {e}")
            lineage.add_metadata("error", str(e))
            raise
        
        # Parse do formato customizado da PBH
        # Formato: <EV=105;HR=...;LT=...>
        try:
            if not text_content:
                raise ValueError("Nenhum conte√∫do recebido da API")
            
            records = []
            
            for line in text_content.strip().split('\n'):
                line = line.strip()
                if not line or not line.startswith('<'):
                    continue
                    
                # Remove < e >
                line = line.strip('<>')
                
                # Parse dos campos CAMPO=VALOR
                record = {}
                for field in line.split(';'):
                    if '=' in field:
                        key, value = field.split('=', 1)
                        record[key.strip()] = value.strip()
                
                if record:
                    records.append(record)
            
            df = pd.DataFrame(records)
            
            if df.empty:
                logger.warning("‚ö†Ô∏è Nenhum dado retornado pela API")
                # Retorna DataFrame vazio com estrutura esperada
                df = pd.DataFrame(columns=['evento', 'horario', 'latitude', 'longitude', 
                                          'numero_veiculo', 'velocidade', 'numero_linha',
                                          'direcao', 'status_veiculo', 'distancia'])
            else:
                # Renomeia colunas para nomes mais descritivos
                column_mapping = {
                    'EV': 'evento',
                    'HR': 'horario',
                    'LT': 'latitude',
                    'LG': 'longitude',
                    'NV': 'numero_veiculo',
                    'VL': 'velocidade',
                    'NL': 'numero_linha',
                    'DG': 'direcao',
                    'SV': 'status_veiculo',
                    'DT': 'distancia'
                }
                df = df.rename(columns=column_mapping)
                
                # Converte tipos de dados num√©ricos
                numeric_columns = ['latitude', 'longitude', 'velocidade', 'numero_veiculo',
                                  'numero_linha', 'direcao', 'status_veiculo', 'distancia', 'evento']
                for col in numeric_columns:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # Adiciona metadados de ingest√£o
            df["_ingestion_timestamp"] = datetime.now()
            df["_source"] = "api_tempo_real"
            
            lineage.add_metadata("records_extracted", len(df))
            logger.info(f"‚úÖ Extra√≠dos {len(df)} registros")
            
            return df
            
        except Exception as e:
            logger.error(f"Erro ao fazer parse dos dados: {e}")
            lineage.add_metadata("error", str(e))
            raise
    
    def load(self, data: pd.DataFrame) -> str:
        """
        Carrega dados na camada Bronze.
        
        Args:
            data: DataFrame a ser carregado
        
        Returns:
            Caminho do arquivo salvo
        """
        return self._save_to_parquet(data, "onibus_tempo_real")


class MCOIngester(BronzeDataIngester):
    """Ingestor de dados do Mapa de Controle Operacional (MCO)."""
    
    def __init__(
        self,
        data_url: Optional[str] = None,
        output_path: str = "./data/bronze"
    ):
        """
        Inicializa o ingestor de dados do MCO.
        
        Args:
            data_url: URL do dataset MCO (ou arquivo local)
            output_path: Caminho de sa√≠da
        """
        super().__init__(output_path)
        self.data_url = data_url
    
    def extract(self, file_path: Optional[str] = None) -> pd.DataFrame:
        """
        Extrai dados do MCO.
        
        Args:
            file_path: Caminho para arquivo CSV local (opcional)
        
        Returns:
            DataFrame com os dados extra√≠dos
        """
        lineage = DataLineage(
            source="PBH MCO Dataset",
            operation="extract_mco"
        )
        
        try:
            if file_path:
                logger.info(f"Lendo arquivo local: {file_path}")
                df = pd.read_csv(file_path, encoding="utf-8", sep=";")
                lineage.add_metadata("source_type", "local_file")
            elif self.data_url:
                logger.info(f"Extraindo dados de: {self.data_url}")
                df = pd.read_csv(self.data_url, encoding="utf-8", sep=";")
                lineage.add_metadata("source_type", "url")
            else:
                raise ValueError("√â necess√°rio fornecer file_path ou data_url")
            
            # Adiciona metadados de ingest√£o
            df["_ingestion_timestamp"] = datetime.now()
            df["_source"] = "mco_dataset"
            
            lineage.add_metadata("records_extracted", len(df))
            logger.info(f"Extra√≠dos {len(df)} registros do MCO")
            
            return df
            
        except Exception as e:
            logger.error(f"Erro ao extrair dados do MCO: {e}")
            lineage.add_metadata("error", str(e))
            raise
    
    def load(self, data: pd.DataFrame) -> str:
        """
        Carrega dados na camada Bronze.
        
        Args:
            data: DataFrame a ser carregado
        
        Returns:
            Caminho do arquivo salvo
        """
        return self._save_to_parquet(data, "mco")


def ingest_all_sources(config: Dict[str, Any]) -> Dict[str, str]:
    """
    Executa ingest√£o de todas as fontes de dados configuradas.
    
    Args:
        config: Dicion√°rio de configura√ß√£o
    
    Returns:
        Dicion√°rio com caminhos dos arquivos salvos
    """
    results = {}
    
    # Ingest√£o de √¥nibus em tempo real
    if config.get("data_sources", {}).get("onibus_tempo_real", {}).get("enabled"):
        try:
            onibus_ingester = OnibusTempoRealIngester(
                api_url=config["data_sources"]["onibus_tempo_real"]["url"],
                output_path=config["layers"]["bronze"]["path"]
            )
            df_onibus = onibus_ingester.extract()
            results["onibus_tempo_real"] = onibus_ingester.load(df_onibus)
        except Exception as e:
            logger.error(f"Falha na ingest√£o de √¥nibus: {e}")
            results["onibus_tempo_real"] = f"ERROR: {e}"
    
    # Ingest√£o do MCO
    if config.get("data_sources", {}).get("mco", {}).get("enabled"):
        try:
            mco_ingester = MCOIngester(
                data_url=config["data_sources"]["mco"]["url"],
                output_path=config["layers"]["bronze"]["path"]
            )
            # Nota: MCO pode precisar de arquivo local, ajustar conforme necess√°rio
            df_mco = mco_ingester.extract()
            results["mco"] = mco_ingester.load(df_mco)
        except Exception as e:
            logger.error(f"Falha na ingest√£o do MCO: {e}")
            results["mco"] = f"ERROR: {e}"
    
    return results
