# Pipeline de Dados de Mobilidade Urbana - Belo Horizonte

[![Python Version](https://img.shields.io/badge/python-3.11%2B-blue.svg)](https://www.python.org/downloads/)
[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Auditoria](https://img.shields.io/badge/auditoria-9.2%2F10-success.svg)](AUDITORIA_TECNICA.md)
[![Status](https://img.shields.io/badge/status-pronto%20para%20produ%C3%A7%C3%A3o-brightgreen.svg)](CHECKLIST_FINAL.md)

Pipeline de dados moderno para extraÃ§Ã£o, transformaÃ§Ã£o e anÃ¡lise de dados de mobilidade urbana de Belo Horizonte, implementando arquitetura Medallion (Bronze-Silver-Gold) com boas prÃ¡ticas de DataOps.

## ğŸ“‹ VisÃ£o Geral

Este projeto implementa um pipeline ETL completo que:

- **Extrai** dados pÃºblicos de mobilidade urbana de Belo Horizonte
- **Armazena** em data lake com arquitetura Medallion
- **Transforma** e limpa os dados com validaÃ§Ãµes de qualidade
- **Carrega** em data warehouse otimizado para anÃ¡lises
- **Automatiza** todo o processo com boas prÃ¡ticas de DataOps

## ğŸ—ï¸ Arquitetura

### Arquitetura Medallion

```
ğŸ“Š Fontes de Dados (APIs Dados Abertos BH)
    â†“
ğŸ¥‰ BRONZE LAYER (Dados Brutos - Parquet)
    â”œâ”€â”€ onibus_tempo_real/
    â””â”€â”€ mco/
    â†“
ğŸ¥ˆ SILVER LAYER (Dados Limpos - Delta Lake)
    â”œâ”€â”€ onibus_posicoes/
    â””â”€â”€ mco_linhas/
    â†“
ğŸ¥‡ GOLD LAYER (MÃ©tricas de NegÃ³cio - Delta Lake)
    â”œâ”€â”€ velocidade_media_por_linha/
    â”œâ”€â”€ onibus_ativos_por_periodo/
    â”œâ”€â”€ cobertura_geografica/
    â””â”€â”€ pontos_criticos_velocidade/
```

### Componentes Principais

- **Bronze**: Dados brutos imutÃ¡veis (Parquet, compressÃ£o Snappy)
- **Silver**: Dados validados e limpos (Delta Lake)
- **Gold**: AgregaÃ§Ãµes e KPIs (Delta Lake)

Para mais detalhes, consulte [ARCHITECTURE.md](docs/ARCHITECTURE.md).

## ğŸš€ Quick Start

### PrÃ©-requisitos

**Sistema**:
- Python 3.11 ou superior
- pip ou conda
- 4GB RAM mÃ­nimo (recomendado 8GB)
- 2GB espaÃ§o em disco

**Conhecimentos**:
- Python bÃ¡sico
- Conceitos de ETL
- SQL/Pandas (desejÃ¡vel)

### InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**
```bash
git clone https://github.com/seu-usuario/bh-mobilidade-pipeline.git
cd bh-mobilidade-pipeline
```

2. **Crie um ambiente virtual**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

3. **Instale as dependÃªncias**
```bash
pip install -r requirements.txt
```

4. **Configure as variÃ¡veis de ambiente**
```bash
cp .env.example .env
# Edite o arquivo .env conforme necessÃ¡rio
```

### Uso BÃ¡sico

#### Executar pipeline completo

```bash
python src/pipeline.py
```

#### Executar camadas especÃ­ficas

```bash
# Apenas Bronze (ingestÃ£o)
python src/pipeline.py --layers bronze

# Silver e Gold (sem ingestÃ£o)
python src/pipeline.py --layers silver gold

# Reprocessar dados existentes
python src/pipeline.py --skip-bronze
```

#### Executar com configuraÃ§Ã£o customizada

```bash
python src/pipeline.py --config config/config.yaml
```

## ğŸ“ Estrutura do Projeto

```
bh-mobilidade-pipeline/
â”œâ”€â”€ config/                  # ConfiguraÃ§Ãµes
â”‚   â””â”€â”€ config.yaml          # Config principal
â”œâ”€â”€ data/                    # Dados (local)
â”‚   â”œâ”€â”€ bronze/              # Camada Bronze
â”‚   â”œâ”€â”€ silver/              # Camada Silver
â”‚   â””â”€â”€ gold/                # Camada Gold
â”œâ”€â”€ docs/                    # DocumentaÃ§Ã£o
â”‚   â””â”€â”€ ARCHITECTURE.md      # Arquitetura detalhada
â”œâ”€â”€ logs/                    # Arquivos de log
â”œâ”€â”€ notebooks/               # Jupyter Notebooks
â”‚   â”œâ”€â”€ 01_exploracao.ipynb
â”‚   â””â”€â”€ 02_analise.ipynb
â”œâ”€â”€ src/                     # CÃ³digo-fonte
â”‚   â”œâ”€â”€ bronze/              # IngestÃ£o de dados
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ ingestion.py
â”‚   â”œâ”€â”€ silver/              # TransformaÃ§Ãµes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ transformation.py
â”‚   â”œâ”€â”€ gold/                # AgregaÃ§Ãµes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ aggregation.py
â”‚   â”œâ”€â”€ utils/               # UtilitÃ¡rios
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ common.py
â”‚   â”‚   â””â”€â”€ data_quality.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ pipeline.py          # Orquestrador principal
â”œâ”€â”€ tests/                   # Testes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_bronze.py
â”‚   â”œâ”€â”€ test_data_quality.py
â”‚   â””â”€â”€ test_utils.py
â”œâ”€â”€ .env.example             # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore               # Arquivos ignorados pelo Git
â”œâ”€â”€ pyproject.toml           # ConfiguraÃ§Ã£o do projeto
â”œâ”€â”€ requirements.txt         # DependÃªncias
â”œâ”€â”€ setup.py                 # Setup do pacote
â””â”€â”€ README.md                # Este arquivo
```

## ğŸ”§ ConfiguraÃ§Ã£o

### Arquivo `config/config.yaml`

```yaml
pipeline:
  name: "bh_mobilidade_urbana_pipeline"
  version: "1.0.0"

data_sources:
  onibus_tempo_real:
    url: "https://temporeal.pbh.gov.br/v1/posicoes"
    enabled: true
  mco:
    url: "https://dados.pbh.gov.br/dataset/mco"
    enabled: true

layers:
  bronze:
    path: "./data/bronze"
    format: "parquet"
  silver:
    path: "./data/silver"
    format: "delta"
  gold:
    path: "./data/gold"
    format: "delta"
```

### VariÃ¡veis de Ambiente

```bash
# .env
ENVIRONMENT=development
LOG_LEVEL=INFO
ENABLE_DATA_QUALITY_CHECKS=true
ENABLE_DATA_LINEAGE=true
```

## ğŸ“Š MÃ©tricas de NegÃ³cio (Gold Layer)

### 1. Velocidade MÃ©dia por Linha
- Velocidade mÃ©dia, mediana, mÃ¡xima e mÃ­nima
- Desvio padrÃ£o
- Agregado por linha e data

### 2. Ã”nibus Ativos por PerÃ­odo
- Total de Ã´nibus Ãºnicos
- DistribuiÃ§Ã£o por hora do dia
- AnÃ¡lise por dia da semana

### 3. Cobertura GeogrÃ¡fica
- Ãrea de cobertura por linha
- Coordenadas centrais
- Densidade de pontos

### 4. Pontos CrÃ­ticos de Velocidade
- IdentificaÃ§Ã£o de gargalos
- Grid geogrÃ¡fico com baixa velocidade
- ClassificaÃ§Ã£o por severidade

## ğŸ§ª Testes

### Executar todos os testes

```bash
pytest
```

### Executar com cobertura

```bash
pytest --cov=src --cov-report=html
```

### Executar testes especÃ­ficos

```bash
pytest tests/test_bronze.py
pytest tests/test_data_quality.py -v
```

## ğŸ” Qualidade de Dados

### ValidaÃ§Ãµes Implementadas

- **Coordenadas geogrÃ¡ficas**: Dentro dos limites de BH
- **Velocidade**: NÃ£o negativa e < 120 km/h
- **Timestamps**: Formato vÃ¡lido e nÃ£o nulo
- **Duplicatas**: IdentificaÃ§Ã£o e remoÃ§Ã£o

### Framework de ValidaÃ§Ã£o

- **Pandera**: Schemas e validaÃ§Ãµes em tempo real
- **Great Expectations**: SuÃ­tes de testes de qualidade
- **Custom Validators**: Regras de negÃ³cio especÃ­ficas

### Score de Qualidade

Cada registro recebe um score de qualidade baseado em:
- Completude dos dados (60%)
- ValidaÃ§Ã£o de coordenadas (40%)

## ğŸ“ˆ DataOps e GovernanÃ§a

### Linhagem de Dados

- Rastreamento completo de origem a destino
- Metadados de transformaÃ§Ãµes
- Timestamps de processamento

### Monitoramento

- Logs estruturados por nÃ­vel
- MÃ©tricas de execuÃ§Ã£o
- Alertas de falhas

### Auditoria

- HistÃ³rico de validaÃ§Ãµes
- Registro de erros
- RelatÃ³rios de qualidade

## ğŸ”„ Agendamento

### Usando Schedule (Python)

```python
import schedule
import time

def job():
    # Executa o pipeline
    pass

schedule.every(15).minutes.do(job)

while True:
    schedule.run_pending()
    time.sleep(1)
```

### Usando Cron (Linux)

```bash
# Executar a cada 15 minutos
*/15 * * * * cd /path/to/project && python src/pipeline.py
```

### Usando Airflow

```python
from airflow import DAG
from airflow.operators.bash import BashOperator

dag = DAG('bh_mobilidade', schedule_interval='*/15 * * * *')

run_pipeline = BashOperator(
    task_id='run_pipeline',
    bash_command='python /path/to/src/pipeline.py',
    dag=dag
)
```

## ğŸš€ Deploy

### OpÃ§Ãµes de Plataforma

1. **Local Development**
   - Python standalone
   - Arquivo local storage

2. **Cloud Platforms**
   - **AWS**: S3 + Glue + Lambda
   - **Azure**: Blob Storage + Databricks + Functions
   - **GCP**: Cloud Storage + Dataproc + Cloud Functions

3. **Databricks**
   - Notebooks nativos
   - Delta Lake otimizado
   - Cluster management

## ğŸ” SeguranÃ§a

- Credenciais em variÃ¡veis de ambiente
- `.env` nÃ£o versionado
- Dados pÃºblicos (sem informaÃ§Ãµes sensÃ­veis)
- Logs sem informaÃ§Ãµes confidenciais

## ğŸ“ Desenvolvimento

### Code Style

```bash
# Formatar cÃ³digo
black src/ tests/

# Verificar estilo
flake8 src/ tests/

# Type checking
mypy src/
```

### Contribuindo

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/nova-feature`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova feature'`)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

## ğŸ“š Recursos

### APIs Utilizadas

- [Portal Dados Abertos BH](https://dados.pbh.gov.br)
- [API Tempo Real - Ã”nibus BH](https://temporeal.pbh.gov.br)

### DocumentaÃ§Ã£o TÃ©cnica

- [Python Pandas](https://pandas.pydata.org/)
- [Delta Lake](https://delta.io/)
- [Pandera](https://pandera.readthedocs.io/)
- [Great Expectations](https://greatexpectations.io/)

